\section{Aproximando funciones con neuronas}

\noindent
En computación, la palabra \emph{aprendizaje} es sinónimo de ``minimización de\
errores'', lo que lleva a cuestionarnos la existencia de una conexión entre\
este fenómeno estadístico con las maneras que, naturalmente, posee el cuerpo humano\
para comprender su medio ambiente. Concretamente, la tarea que tiene el computólogo\
ante sí consiste en transformar los problemas de aprendizaje automático en\
equivalentes compatibles con el funcionamiento del sistema nervioso humano. Dadas,\
las evidencias empíricas sobre el buen desempeño del cerebro humano en su cotidianidad,\
nos inspiramos en el comportamiento biológico del mismo para construir arquitecturas\
cuyo objetivo será el de aproximar funciones complejas, creando un novedoso paradigma de cómputo.\par
Con el fin de ilustrar al buen \emph{performance} del cerebro humano, consideremos a las\
habilidades de reconocimiento perceptivo. Mientras una persona tarda de $100$ a $200$ $ms$\
en detectar un rostro familiar, una computadora con suficiente poder dura mucho más.\cite{haykin2009}\
En contraste, se sabe que individualmente, una neurona es mucho más lenta que una\
compuerta lógica: mientras que ésta última tarda pocos nanosegundos en \emph{conmutar},\
a la primera mencionada le puede tomar varios milisegundos en reaccionar a un estímulo.

\subsection{Inspiración a partir de la biología}

\noindent
El sistema nervioso es una red \emph{paralela} y \emph{auto-organizada}. $86$ mil\
millones de neuronas (aproximadamente) \cite{website:nature:scitable} conforman una arquitectura que\
funciona a través de la emisión de pulsos eléctricos y la reacción ante ellos. Dos neuronas\
están conectadas entre sí por medio de estructuras conocidas como \emph{sinapsis},\
a través de las cuales se transmiten señales eléctricas y químicas.\par
El cerebro es, además, un órgano que se adapta a las condiciones de su ambiente.\
Evidencia de ello es la creación de conexiones sinápticas entre neuronas (previamente\
desconectadas) y la modificación del mecanismo de las sinapsis existentes. Una vez que una\
neurona haya emitido una señal eléctrica, las adyacentes reciben la \emph{``información''}\
por medio de canales de transimisión llamados \emph{dendritas}. Estos impulsos son llevados\
hasta el \emph{cuerpo} de la neurona para su procesamiento y, posteriormente, una reacción\
es transmitida a través del \emph{axón} de la célula. Los organelos mencionados anteriormente\
constituyen las principales partes de la neurona que habrán de servir como estructuras\
fundamentales de las arquitecturas de aprendizaje a presentar en las siguientes secciones.\cite{rojas1996}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{neurona}
  \caption{Ilustración de las partes más importantes de una neurona humana.
    (Tomado de (\url{https://psi121f.wordpress.com/2016/07/03/la-estructura-de-la-neurona-2/}))}
\end{figure}


\subsection{El modelo de cómputo neuronal}

\noindent
Dadas varias entradas eléctricas, una neurona deberá de ajustar su reacción de manera proporcional\
a la intensidad de las dichas señales. Para formalizar el comportamiento de una neurona en términos\
matemáticos (y, por ende, computacionales), es imprescindible caracterizar las reglas que sigue una neurona\
para componer sus señales de entrada y manejarlas ``globalmente'' mediante una función. Cabe destacar\
que, por simplicidad y elegancia de los modelos a estudiar, resulta importante conocer la \emph{sincronía}\
de la transmisión de información así como la presencia o ausencia de ciclos o bucles. Todo esto se puede\
englobar en un conjunto de características topológicas y algorítmicas que constituyen a una \emph{red neuronal artificial}\
(ANN, por sus siglas en inglés; en adelante, abreviaremos ANN con NN).\par
Con respecto a la representación interna del conocimiento de una NN, estamos ante un conjunto de modelos\
que buscarán modelar la información de manera \emph{asociativa}; análogamente al cerebro humano. Por ende,\
se requieren modelos que \emph{relacionen} clasificaciones de objetos similares con representaciones internas\
similares. Con el fin de asegurarnos de ello, presentaremos más adelante una completa sección\
acerca de métodos para medir similitud. Una consecuencia importante de esto es que,\
de manera contraria, si se desea que dos objetos sean distintamente clasificados,\
entonces se les deben de dar dos representaciones totalmente diferentes.\par
Independientemente de la tarea que se desea aprender, siempre existirá alguna característica cuya importancia\
define el veredicto de la NN. Una forma de respaldar este hecho consiste en dedicar un gran\
número de neuronas a la identificación de dicha característica. Con ello, se aumenta la\
precisión de la NN en su toma de decisiones, contrastando con la existencia de neuronas defectuosas.\par
Finalmente, la existencia de un \emph{zoológico de redes neuronales} se justifica con la tendencia\
que se sigue a diseñar modelos específicos para ciertas tareas. Si se sabe información \textit{a priori}\
sobre los datos a procesar, es mejor integrarlas en el diseño de la arquitectura a dejar que ésta\
los aprenda durante el entrenamiento. Después de todo, en el cuerpo humano existen una gran cantidad\
de estructuras neuronales especializadas para funciones como visión y audición, muy distintas a otras\
presentes en el cerebro.

\subsection{El perceptrón de Rosenblatt}

\noindent
La idea de cómputo a través de redes neuronales fue tan trascendente desde su concepción, que, en 1943,\
McCulloch y Pitts la incluyeron en el mismo artículo que introdujo a los sistemas de transición con un número\
finito de estados \cite{mcculloch:pitts}. Sin embargo, fue Rosenblatt, en 1958, quien propuso el primer modelo de aprendizaje supervisado para una NN.\
El perceptrón es la red neuronal más simple y, muchas veces, será uno de los bloques básicos de arquitecturas\
más complejas. La intuición matemática detrás de su estructura consiste en \emph{separar linealmente} las entradas dadas en\
dos clases, lo cual se logra mediante un aprendizaje que va ajustando pesos de acuerdo a las salidas esperadas.\par

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{perceptron}
  \caption{El perceptrón de Rosenblatt es un modelo gráfico cuya salida depende de las contribuciones
    lineales de cada una de las entradas, de un sesgo y de una función no lineal de activación.
    (Tomado de \cite{haykin2009})}
  \label{perceptron-fig}
\end{figure}


De acuerdo a la Figura \ref{perceptron-fig}, el perceptrón opera de la siguiente manera: dados los valores\
de entrada $x_1, x_2,\ldots, x_m$ y los pesos $w_1, w_2,\ldots, w_m$,
\begin{itemize}
\item se calcula una suma ponderada con los $m$ pesos (\emph{sinápticos}) del modelo,
\item a la suma anterior, se le añade un valor de \emph{``tendencia''} conocido como \emph{sesgo}\
  (\emph{bias} en inglés); formalmente:
  \begin{equation}
    v = b + \sum_{i=1}^{m} w_ix_i
  \end{equation}
\item finalmente, la salida del perceptrón se calcula aplicando una función \emph{de activación} a $v$:
    \begin{equation}
      y = \Phi(v).
    \end{equation}
\end{itemize}

La función de activación juega el papel directo de clasificador: su salida debe decidir si la entrada\
pertenece, o no, a cierta clase. De ahí que en la mayoría de los casos, se trata de una función cuya imagen\
es $\{0,1\}$ o $\{-1,1\}$. Por otra parte, dado que estamos definiendo al perceptrón con operaciones\
aritméticas como sumas y productos, cabe recalcar que las entradas deben ser una \emph{abstracción numérica}\
del elemento del entorno a clasificar; muchas veces ésta se compone de un vector de valores reales. Ello implica\
la existencia de un \emph{umbral} $U$ (\emph{threshold} en inglés) que divida los posibles valores de $v$\
en dos, permitiendo su clasificación binaria. A continuación, se define la \emph{función escalón}, valuada\
en $\{-1,1\}$ (una posible función de activación):
\begin{equation}
  \Phi(v) =
  \begin{cases}
    1 & \text{si } v > U\\
    -1 & \text{en otro caso}
  \end{cases}
\end{equation}
Geométricamente, el perceptrón genera un \emph{hiperplano} que, con los pesos adecuados logrará dividir\
las entradas de manera que cada entrada correspondiente a una cierta clase quede dentro de \emph{una y sólo una}\
partición del espacio multidimiensional en cuestión.\par
El hecho de que estamos usando funciones de activación valuadas de manera binaria nos invita a explorar el\
cómputo \emph{neuronal} de las diversas funciones lógicas. Como ejemplo, está la clasificación de\
la función \verb+OR+ en la Figura \ref{or-fig}. En este caso, se tienen entradas de dos dimensiones, por lo que es\
posible visualizarlas gráficamente; en la práctica, se trabaja con un gran número de dimensiones, de lo cual\
se deduce la importancia de tomarán algunos métodos de reducción dimensional para el éxito de los algoritmos\
de entrenamiento de modelos más complejos.\par

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{or}
  \caption{La función lógica \texttt{OR}, en su versión bidimensional, tiene cuatro posibles salidas:
    tres de ellas arrojan un valor positivo, mientras que la otra es negativa. Claramente, se trata de una función
    separable mediante una recta.
    (Elaboración propia.)}
  \label{or-fig}
\end{figure}

Para finalizar esta sección, cabe destacar que la función de activación ($signo$) que acaba de ser presentada\
no es la única. En realidad, se buscan opciones más suavizadas y, sobre todo, diferenciables con el fin de\
optimizar los parámetros de modelos más complejos.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{activations}
  \caption{Las funciones de activación más comunes.
    (Tomado de (\url{https://www.slideshare.net/SungJuKim2/multi-layer-perceptron-back-propagation}))}
\end{figure}

\subsection{El perceptrón multicapa}

\noindent
No todos los patrones de datos son linealmente separables. Por ejemplo, la función lógica \verb+XOR+, contiene\
salidas cuyos valores no pueden ser divididos en dos partes del plano sin ser mezclados. Por consiguiente,\
es necesario aumentar la capacidad de cómputo del perceptrón. Combinaremos, ahora, varios perceptrones\
en una \emph{red neuronal de propagación hacia adelante}, estructurando el cómputo en distintas\
\emph{capas ocultas}.\par
El flujo de la información irá de capa en capa, en una sola dirección hasta llegar a una \emph{capa de salida}.
Cada capa oculta consta de un conjunto de neuronas, sin conexiones entre ellas, pero totalmente conectadas a la\
capa inmendiatamente anterior y posterior. A este modelo se le conoce comúnmente como\
\textbf{perceptrón multicapa} (\emph{MLP} por sus siglas en inglés) y es el punto de partida de la rama\
del aprendizaje automático conocida como \textbf{aprendizaje profundo} (\emph{deep learning} por sus siglas en inglés).

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{mlp}
  \caption{Arquitectura de un MLP con dos capas ocultas.
    (Tomado de \cite{haykin2009}.)}
\end{figure}

El flujo del cómputo en un MLP puede ser pensado como la composición de las capas ocultas, vistas como funciones que\
procesan los datos de entrada. Formalmente, dado un vector de entrada $\mathbf{x}$, la primera capa oculta $h_1$\
calcula su valor utilizando una matriz de pesos $\mathbf{W_1}$, un sesgo $\mathbf{b_1}$ de la siguiente manera:
\begin{equation}
  h_1 = \sigma\left(\mathbf{W_1}^\top \mathbf{x} + \mathbf{b}_1\right), \label{mlpfirst}
\end{equation}
donde $\sigma$ es una función de activación no lineal y \emph{diferenciable}.\
Las dimensiones de $\mathbf{W}$ corresponden a la entrada y al número de neuronas en la capa $h_1$.\
La $i+1$-ésima capa oculta de un MLP actualiza su valor a partir de la $i$-ésima mediante
\begin{equation}
  h_{i+1} = \sigma\left(\mathbf{W_{i+1}}^\top h_i + \mathbf{b}_{i+1}\right), \label{mlphidden}
\end{equation}
es decir, cada capa tendrá una matriz de pesos y un vector de sesgos propio. La salida, suponiendo que\
hay $m$ capas ocultas, obtiene su valor con la ecuación
\begin{equation}
  \hat{y} = \sigma\left(\mathbf{W_{m+1}}^\top h_m + \mathbf{b}_{m+1}\right). \label{mlpoutput}
\end{equation}
Obsérvese que es posible que $\hat{y}$ tenga más de una dimensión, entonces, cada uno de los valores de dicho\
vector codificará ``la existencia'' de alguna característica en particular. Esto es muy útil para aprender\
clasificadores cuyo codominio es mayor al conjunto binario.\par
En ocasiones, al subgrafo dirigido formado por las capas ocultas $h_i$ y $h_{i+1}$ se le conoce como\
\emph{capa densa} o \emph{totalmente conectada} y es común encontrarla en arquitecturas de mayor complejidad\
y especialización. El MLP está dentro del ``estado del arte'' de los algoritmos de clasificación automática y\
será parte fundamental de los dos modelos neuronales que compone a la arquitectura de procesamiento de imágenes\
y generación de lenguaje de la presente tesis. La parte más importante de ello recae en su algoritmo de\
entrenamiento.

\subsection{Retroalimentación para aprender}

\noindent
Como cualquier arquitectura de aprendizaje supervisado, a un MLP debe de asociarse una función objetivo\
(o de \emph{error}) $J$, que le permita estructurar un entrenamiento. La no-linealidad de las funciones de\
activación causan que no se garantice la convexidad por parte de las funciones de error más comunes. Al\
no existir un método analítico para encontrar mínimos sin importar la elección de $J$, el entrenamiento\
de un MLP (y, en general, de una red neuronal profunda) se basa en métodos iterativos que van optimizando\
los parámetros de la arquitectura, tales como optimizadores basados en gradientes.\par
Para estar \textit{ad hoc} a la práctica, presentaremos un algoritmo de \emph{descenso por el gradiente estocástico},
el cual, a pesar de no garantizar la convergencia hacia un punto mínimo, funciona bien sabiendo\
escoger los valores adecuados de inicialización. Dada la salida $\hat{y}$ establecida en la Ecuación\
\ref{mlpoutput}, definimos la función de error total del MLP como
\begin{equation}
  J(\bm{\theta}) = \frac{1}{2n} \sum_{i=1}^n (\hat{y}_i - y_i)^2 \label{mlperror}
\end{equation}
donde $\bm{\theta}$ es el conjunto de los parámetros del MLP, es decir, $\bm{\theta} := \bigcup\{\bm{W}_i, \bm{b}_i\}_{i=1}^n$;\
$n$ es el número de capas del MLP; e $y_i$ es la $i$-ésima etiqueta\
de entrenamiento. A esta función se le conoce como \emph{error cuadrático medio}; en general,
una función de error $J$ se define conceptualmente como la \textbf{entropía cruzada}\
de las distribuciones de los datos ($y \equiv p_{\text{DATOS}}$) y del modelo ($\hat{y} \equiv \hat{p}_{\text{MLP}}$).\
Esto es, la esperanza de la verosimilitud logarítmica negativa del modelo%
\footnote{
  Suponiendo que $p_{\text{DATOS}}$ es una distribución \emph{normal},\
  se puede demostrar la igualdad entre las Ecuaciones \ref{crossentropydef} y \ref{mlperror}.
 }:
\begin{align}
  J(\bm{\theta}) &=  - \mathbb{E}_{p_{\text{DATOS}}} \log\hat{p}_{\text{MLP}}(\bm{x}) \label{crossentropydef}\\
  &= - \sum_i (y_i\log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)). \label{crossentropy}
\end{align}
Esta función es particularmente útil si se desea interpretar las salidas del MLP como probabilidades, ya que,\
$\sum_i \hat{y}_i = 1$ y $0 < \hat{y}_i < 1 $ para $1 \leq i \leq n$. Cabe destacar que ambas funciones\
son completamente diferenciables y los procedimientos de optimización, basados en gradientes, son invariantes.

\subsubsection{Propagación hacia atrás}

\noindent
Cualquier algoritmo de optimización basado en gradientes necesita de un método (\emph{eficiente}) para calcular\
las derivadas de los parámetros del modelo. En la literatura de aprendizaje profundo el cálculo de $\hat{y}$\
muchas veces se le conoce como \emph{propagación hacia adelante}. Esto da lugar al algoritmo de\
\textbf{propagación hacia atrás} (o \emph{retropropagación}) que permite que el error resultante $J$ se\
propague de adelante hacia atrás, a través de todos los parámetros del MLP. El objetivo principal de este\
algoritmo será el cómputo del gradiente del modelo $J$ con respecto a los parámetros $\bm{\theta}$, es decir,\
$\nabla_{\bm{\theta}} J(\bm{\theta})$.\par
Para empezar, el algoritmo de propagación hacia atrás se fundamenta en la regla de la cadena del\
\emph{cálculo infinitesimal} para el cómputo de derivadas de composiciones de funciones:\
dados $x \in \mathbb{R}$, $y = g(x)$ y $z = f(g(x)) = f(y)$, la derivada de $z$ con respecto a $x$ se obtiene por
\begin{equation}
  \frac{dz}{dx} =  \frac{dz}{dy} \frac{dy}{dx}.
\end{equation}
Esta noción se generaliza a funciones que involucran la composición de más de dos funciones:\
si
\begin{itemize}
\item $\mathbf{x} \in \mathbb{R}^m$,
\item $\mathbf{y} \in \mathbb{R}^n$,
\item $g: \mathbb{R}^m \to \mathbb{R}^n$,
\item $f: \mathbb{R}^n \to \mathbb{R}$,
\item $\mathbf{y} = g(\mathbf{x})$ y
\item $z = f(\mathbf{y})$, entonces
\end{itemize}
\begin{equation}
  \frac{\partial z}{\partial x_i} =
  \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}. \label{multichain}
\end{equation}
En forma vectorial, la Ecuación \ref{multichain} se escribe como
\begin{equation}
  \nabla_{\mathbf{x}} z =
  \left(\frac{\partial y}{\partial x}\right)^\top \nabla_{\mathbf{y}} z. \label{vectchain}
\end{equation}\par
De la Ecuación \ref{vectchain} vemos que la expresión $\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$\
es la matriz \emph{jacobiana} (de $n \times m$) de $g$, es decir, el gradiente de una variable vectorial\
$\mathbf{x}$ se obtiene multiplicando dicho jacobiano por el gradiente $\nabla_{\mathbf{y}} z$.\
Para cada operación en el \emph{grafo dirigido}, definido en un MLP, el algoritmo de propagación hacia atrás realizará\
un producto entre jacobianos y gradientes.\par
En la práctica, el cómputo de estos gradientes es invariante si en vez de vectores tenemos tensores.\
Intuitivamente, uno puede pensar que, previamente a correr propagación hacia atrás, cualquier tensor\
de dimensiones $n \times m \times p$ se \emph{aplane} a un vector de dimensión $1 \times nmp$. Tras el\
cálculo de los gradientes, los vectores aplanados vuelven a su representación \emph{tensorial}.\par
Mediante la regla de la cadena, es posible desarrollar una expresión para el cálculo del gradiente\
de $J$ con respecto a todos los parámetros $\bm{\theta}$ del MLP. Sin embargo, una implementación\
\emph{ingenua} involucraría que el cálculo de muchas subexpresiones se repita un número considerable de veces.\
En muchas ocasiones, calcular la misma expresión más de una vez resulta una pérdida de tiempo y/o memoria,\
la cual muchas veces puede significar un gasto de orden exponencial con respecto al número de parámetros.\par
En términos formales, consideremos a $u$ como una neurona de la capa $k+1$ de un MLP arbitrario y que recibe como entradas\
las salidas de las neuronas $v_1, v_2, \ldots, v_m$ de la capa $k$. De acuerdo a la Ecuación \ref{vectchain},\
el gradiente de la función de error $J$ con respecto a $u$ se obtiene mediante
\begin{equation}
  \nabla_{u} J =
  \left(\frac{\partial \bm{v}}{\partial u}\right)^\top \nabla_{\bm{v}} J, \label{neurongradient}
\end{equation}
donde $\bm{v} = (v_1, v_2, \ldots, v_m)$. Este es un proceso recursivo que inicia con el cómputo del gradiente\
para la última capa, en donde $\frac{\partial J}{\partial J_i} = 1$ para toda neurona de salida $J_i$.\
Nótese que si para obtener $J$ se requirieron $n$ nodos y cada uno de éstos requiere un tiempo constante de\
cómputo más $m$ arcos, entonces calcular $\nabla_{u} J$ requiere al menos $\Omega(m)$ cómputos y no es posible\
obtener los gradientes de una manera más rápida.\par
Remembrando la terminología establecida a partir de las Ecuaciones \ref{mlpfirst} y \ref{mlphidden}, sea
\begin{equation}
  a_k = \bm{b}_k + \bm{W}_kh_{k-1} \label{mlplinearcomb}
\end{equation}
la ecuación que define la combinación lineal que determina las salidas de la $k$-ésima capa de un MLP. En lo\
secuente, utilizaremos a la función de error por entropía cruzada (Ecuación \ref{crossentropydef}) y asumimos,\
sin pérdida de generalidad, como funciones de activación $\sigma = softmax$ para la última capa\
y $\sigma = \tanh$ para las capas ocultas. Asumiendo que hay $L$ capas en un MLP,\
es importante enfatizar en las siguientes observaciones:
\begin{align}
  \frac{\partial(-\log \hat{y})}{\partial a_{L, i}} &= \hat{y}_i - \mathbbm{1}_{y=i}, \label{derivlosscomb}\\
  \frac{\partial \tanh(u)}{\partial u} &= 1 - \tanh(u)^2. \label{derivtanh}
\end{align}
Resulta lógico entender que la Ecuación \ref{derivlosscomb} calcula la diferencia que existe entre las salidas\
del MLP y las etiquetas del conjunto de datos. Por otro lado, la Ecuación \ref{derivtanh} calcula las\
derivadas parciales de la función tangente hiperbólica aplicada a cualquier tensor real $u$. Obsérvese\
que de la expresión $a_{L, i}$ nos referimos a la última capa de un MLP mediante su índice, notación\
que hará más fácil escribir el siguiente procedimiento.\par
\begin{itemize}
\item Para la función de error, $\frac{\partial J}{\partial J} = 1$.
\item El gradiente del error con respecto a cada entrada de la última capa:
  \begin{equation}
    \frac{\partial J}{\partial a_{L, i}} =
    \frac{\partial J}{\partial J} \frac{\partial J}{\partial a_{L, i}} =
    \hat{y}_i - \mathbbm{1}_{y=i}. \label{derivtanh}
  \end{equation}
\item Para cada capa, desde $k = L$ bajando hasta 1:
  \begin{itemize}
  \item calculamos el gradiente con respecto a los sesgos:
    \begin{equation}
      \frac{\partial J}{\partial \bm{b}_{k, i}} =
      \frac{\partial J}{\partial a_{k, i}} \frac{\partial a_{k, i}}{\partial \bm{b}_{k, i}} =
      \frac{\partial J}{\partial a_{k, i}}, \label{biasgradient}
    \end{equation}
  \item calculamos el gradiente con respecto a los pesos:
    \begin{equation}
      \frac{\partial J}{\partial \bm{W}_{k, i, j}} =
      \frac{\partial J}{\partial a_{k, i}} \frac{\partial a_{k, i}}{\partial \bm{W}_{k, i, j}} =
      \frac{\partial J}{\partial a_{k, i}} h_{k-1, j}, \label{biasgradient}
    \end{equation}
  \item propagamos el gradiente hacia una capa inferior; para $k > 1$:
    \begin{align}
      \frac{\partial J}{\partial h_{k-1, j}} &=
      \sum_i \frac{\partial J}{\partial a_{k, i}} \frac{\partial a_{k,i}}{\partial h_{k-1, j}} =
      \sum_i \frac{\partial J}{\partial a_{k,i}} \bm{W}_{k, i, j}, \label{hiddenbackprop}\\
      \frac{\partial J}{\partial a_{k-1, j}} &=
      \frac{\partial J}{\partial h_{k-1, j}} \frac{\partial h_{k-1, j}}{\partial a_{k-1, j}} =
      \frac{\partial J}{\partial h_{k-1, j}} (1 - h_{k-1, j}^2). \label{combbackprop}
    \end{align}
  \end{itemize}
\end{itemize}

\subsubsection{Descenso por el gradiente}

\noindent
Una vez obtenido el diferencial por el cual se va a optimizar un MLP con respecto a la función\
de error, lo que procede es actualizar los parámetros de la red neuronal con una cantidad adecuada.\
\emph{Descenso por el gradiente} es un algoritmo que minimiza la función de error $J(\bm{\theta})$\
de una red neuronal (en general) mediante la actualización de los parámetros $\bm{\theta} \in \mathbb{R}^d$\
en dirección opuesta al gradiente $\nabla_{\bm{\theta}}J(\bm{\theta})$.\par
En su versión más sencilla, este algoritmo intenta encontrar un mínimo (local) mediante un proceso\
iterativo en el que el gradiente $\nabla_{\bm{\theta}}J(\bm{\theta})$ se calcula y se actualizan \emph{todos}\
los parámetros en proporción con el \emph{meta}-parámetro $\eta$, conocido como \textbf{tasa de aprendizaje}.\
El número $\eta$ determina el \textit{tamaño de los pasos} que se deben tomar para llegar al\
mínimo local. Así,
\begin{equation}
  \bm{\theta} = \bm{\theta} - \eta \nabla_{\bm{\theta}}J(\bm{\theta}). \label{vanillagd}
\end{equation}\par
La Ecuación \ref{vanillagd} implica que hay que calcular los gradientes por cada parámetro del\
conjunto de datos para realizar \emph{una sola} actualización. Esto deriva en un procedimiento\
realmente lento e incluso intratable para conjuntos de datos que no quepan en memoria. La lentitud\
se justifica por el cómputo redundante de varios gradientes para entradas similares antes\
de actualizar cada parámetro. Para sobreponerse a este problema, se da lugar al algoritmo de\
\emph{descenso por el gradiente estocástico} (\emph{SGD}, por sus siglas en inglés)\
en el cual se realiza una actualización por cada ejemplar $(x^{(i)}, y^{(i)})$ del conjunto de datos:
\begin{equation}
  \bm{\theta} = \bm{\theta} - \eta \nabla_{\bm{\theta}}J(\bm{\theta}; x^{(i)}; y^{(i)}). \label{sgd}
\end{equation}
Potencialmente, SGD permite llegar a mejores mínimos locales a los que converge la primera\
versión descrita. Por otro lado, es más probable que no se pueda llegar al mínimo global, dada\
la fluctuación que puede haber en las primeras actualizaciones. Sin embargo, se puede demostrar\
que si la tasa de aprendizaje es suficientemente pequeña, es casi segura la convergencia hacia\
un mínimo local.\par
En la práctica, la versión implementada en la mayoría de las bibliotecas de aprendizaje automático\
incorpora lo mejor de las Ecuaciones \ref{vanillagd} y \ref{sgd}:
\begin{equation}
  \bm{\theta} = \bm{\theta} - \eta \nabla_{\bm{\theta}}J(\bm{\theta}; x^{[i:i+n]}; y^{[i:i+n]}). \label{minibatchgd}
\end{equation}
Cada actualización propuesta por la Ecuación \ref{minibatchgd} se da en un rango de los datos,\
conocido como \textbf{mini-lote} o, simplemente, \emph{lote}. Esto permite que la covergencia se de\
de manera más estable. Además, el tamaño del lote $n$ usualmente varía entre 50 y 256. El número\
de veces que el algoritmo ve el conjunto de datos entero se le llama número de \textbf{épocas}\
Por otro lado, definimoas al número de \emph{iteraciones} del algoritmo,\
contando el número de veces que cada lote pasa por el mismo. El Procedimiento \ref{minibatchgdcode}\
describe una implementación del algoritmo de descenso por el gradiente utilizado en muchas bibliotecas\
actuales de aprendizaje automático.
\begin{lstlisting}[language=Python,
    caption={
      Implementación del algoritmo de descenso por el gradiente utilizando mini-lotes, en lenguaje
      Python. Antes de iterar sobre todos los lotes, se mezclan los datos de manera aleatoria.
    },
   label=minibatchgdcode
  ]
  for i in range(num_epocas):
  np.random.shuffle(datos)
  for lote in get_lotes(datos, tamanyo_lote=50):
    params_grad = evaluar_gradiente(funcion_error, lote, params)
    params = params - tasa_aprendizaje * params_grad
\end{lstlisting}\par
Visualmente el algoritmo de descenso por el gradiente se puede entender mediante la Figura \ref{visualgd}.\
El componente restante, y que es motivo de investigación, es la manera en la que la tasa de aprendizaje\
se calcula. Para ello, se proponen varios \emph{optimizadores} descritos en \cite{DBLP:journals/corr/Ruder16};\
la elección de su uso depende fuertemente del diseño de la arquitectura en cuestión.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{visualgd}
  \caption{
    La optimización realizada por el algoritmo SGD con mini-lotes. Se muestra
    el comportamiento de distintos optimizadores.
    (Tomado de \cite{DBLP:journals/corr/Ruder16}.)
  }
  \label{visualgd}
\end{figure}
