{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Deep Learning Pipeline for Meme captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import os, codecs\n",
    "from utils.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Keras Inception definition\n",
    "See [`inception_v3.py`](utils/inception_v3.py) for more info on the specification.\n",
    "The two last layers are ignored due to implementation issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Flatten at 0x7eff59d6ed30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, _ = InceptionV3(include_top=True, weights='imagenet')\n",
    "\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "\n",
    "#with tf.variable_scope(\"inception_v3\") as scope:\n",
    " #   writer = tf.summary.FileWriter('./cnn', cnn_graph)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A data sample will extracted from [`meme_characters/`](meme_characters/). This info must\n",
    "be crawled!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The `stats` function will compute the statistics from the existing dataset in\n",
    "[`meme_characters/`](meme_characters/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from utils.meme_stats import stats, sizeof_fmt\n",
    "\n",
    "global_dir = '10kmc/part-0-to-10000/'\n",
    "# ncaptions, nmeme_characters, nwords, nchars, total_size = stats(global_dir, False)\n",
    "# print('total number of captions', ncaptions)\n",
    "# print('total number of meme characters', nmeme_characters)\n",
    "# print('total number of words:', nwords)\n",
    "# print('total number of characters:', nchars)\n",
    "# print('total size:', sizeof_fmt(total_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we get a proportion of the data and we encode it using a CNN and sequence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_size: 358410\n",
      "ub: 89602.5\n",
      "1\n",
      "2\n",
      "vocabulary length: 3405\n",
      "total sequences: 2587\n",
      "total chars: 3405\n",
      "Vectorization...\n",
      "(2587, 2)\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import get_data\n",
    "\n",
    "images_captions, voca = get_data(global_dir, model, quantity=0.25)\n",
    "print(np.shape(images_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Model\n",
    "\n",
    "input = Input(batch_shape=(32, 10, 1))\n",
    "lstm_layer = LSTM(10, stateful=True)(input)\n",
    "\n",
    "model = Model(input, lstm_layer)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "hidden_states = K.variable(value=np.random.normal(size=(32, 10)))\n",
    "cell_states = K.variable(value=np.random.normal(size=(32, 10)))\n",
    "\n",
    "model.layers[1].states[0] = hidden_states\n",
    "model.layers[1].states[1] = cell_states "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Interactive session creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## TRÃˆS IMPORTANT !\n",
    "\n",
    "Images and captions are stored in a tensor called __`images_captions`__, which is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def batch_with_dynamic_pad(images_and_captions,\n",
    "                           batch_size,\n",
    "                           queue_capacity,\n",
    "                           add_summaries=True):\n",
    "  \"\"\"Batches input images and captions.\n",
    "  This function splits the caption into an input sequence and a target sequence,\n",
    "  where the target sequence is the input sequence right-shifted by 1. Input and\n",
    "  target sequences are batched and padded up to the maximum length of sequences\n",
    "  in the batch. A mask is created to distinguish real words from padding words.\n",
    "  Example:\n",
    "    Actual captions in the batch ('-' denotes padded character):\n",
    "      [\n",
    "        [ 1 2 5 4 5 ],\n",
    "        [ 1 2 3 4 - ],\n",
    "        [ 1 2 3 - - ],\n",
    "      ]\n",
    "    input_seqs:\n",
    "      [\n",
    "        [ 1 2 3 4 ],\n",
    "        [ 1 2 3 - ],\n",
    "        [ 1 2 - - ],\n",
    "      ]\n",
    "    target_seqs:\n",
    "      [\n",
    "        [ 2 3 4 5 ],\n",
    "        [ 2 3 4 - ],\n",
    "        [ 2 3 - - ],\n",
    "      ]\n",
    "    mask:\n",
    "      [\n",
    "        [ 1 1 1 1 ],\n",
    "        [ 1 1 1 0 ],\n",
    "        [ 1 1 0 0 ],\n",
    "      ]\n",
    "  Args:\n",
    "    images_and_captions: A list of pairs [image, caption], where image is a\n",
    "      Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n",
    "      any length. Each pair will be processed and added to the queue in a\n",
    "      separate thread.\n",
    "    batch_size: Batch size.\n",
    "    queue_capacity: Queue capacity.\n",
    "    add_summaries: If true, add caption length summaries.\n",
    "  Returns:\n",
    "    images: A Tensor of shape [batch_size, height, 1].\n",
    "    input_seqs: An int32 Tensor of shape [batch_size, padded_length].\n",
    "    target_seqs: An int32 Tensor of shape [batch_size, padded_length].\n",
    "    mask: An int32 0/1 Tensor of shape [batch_size, padded_length].\n",
    "  \"\"\"\n",
    "  enqueue_list = []\n",
    "  for image, caption in images_and_captions:\n",
    "    caption_length = tf.shape(caption)[0]\n",
    "    input_length = tf.expand_dims(tf.subtract(caption_length, 1), 0)\n",
    "\n",
    "    input_seq = tf.slice(caption, [0], input_length)\n",
    "    target_seq = tf.slice(caption, [1], input_length)\n",
    "    indicator = tf.ones(input_length, dtype=tf.int32)\n",
    "    img = image.flatten()\n",
    "    # print(np.shape(img))\n",
    "    enqueue_list.append([img, input_seq, target_seq, indicator])\n",
    "    \n",
    "\n",
    "  images, input_seqs, target_seqs, mask = tf.train.batch_join(\n",
    "      enqueue_list,\n",
    "      batch_size=batch_size,\n",
    "      capacity=queue_capacity,\n",
    "      dynamic_pad=True,\n",
    "      name=\"batch_and_pad\")\n",
    "  print(tf.shape(images))\n",
    "\n",
    "  if add_summaries:\n",
    "    lengths = tf.add(tf.reduce_sum(mask, 1), 1)\n",
    "    tf.summary.scalar(\"caption_length/batch_min\", tf.reduce_min(lengths))\n",
    "    tf.summary.scalar(\"caption_length/batch_max\", tf.reduce_max(lengths))\n",
    "    tf.summary.scalar(\"caption_length/batch_mean\", tf.reduce_mean(lengths))\n",
    "    \n",
    "  # tf.reshape(images, (tf.shape(images)[0], tf.shape(images)[2]))  \n",
    "  return images, input_seqs, target_seqs, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## (LSTM) Model Parameter Definitions\n",
    "\n",
    "Variables are defined, and embeddings are built up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "\n",
    "# To match the \"Show and Tell\" paper we initialize all variables with a\n",
    "# random uniform initializer.\n",
    "initializer_scale = 0.08\n",
    "initializer = tf.random_uniform_initializer(\n",
    "        minval=-initializer_scale,\n",
    "        maxval=initializer_scale)\n",
    "images, input_seqs, target_seqs, input_mask = (\n",
    "          batch_with_dynamic_pad(images_captions[:200],\n",
    "                                 batch_size=100,\n",
    "                                 queue_capacity=200))\n",
    "with tf.variable_scope(\"seq_embedding\"), tf.device(\"/cpu:0\"):\n",
    "    embedding_map = tf.get_variable(\n",
    "        name=\"map\",\n",
    "        shape=[len(voca), embedding_size],\n",
    "        initializer=initializer)\n",
    "    seq_embeddings = tf.nn.embedding_lookup(embedding_map, input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#images = tf.reshape(images, (2,))\n",
    "print(tf.shape(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Map inception output into embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"image_embedding\") as scope:\n",
    "    image_embeddings = tf.contrib.layers.fully_connected(\n",
    "        inputs=images,\n",
    "        num_outputs=100,\n",
    "        activation_fn=None,\n",
    "        weights_initializer=initializer,\n",
    "        biases_initializer=None,\n",
    "        scope=scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A [`LSTMStateTuple`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple) will be needed to store all the (_batched_) embeddings obtained from the ConvNet, in order to initialize our RecurrentNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMStateTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## LSTM Specification\n",
    "\n",
    "A (_training_) LSTM net is specified given a [`BasicLSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell), a `LSTMStateTuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=100, state_is_tuple=True)\n",
    "state = tf.Variable(image_embeddings,\n",
    "                    expected_shape=tf.shape(image_embeddings),\n",
    "                    trainable=True)\n",
    "initial_state = LSTMStateTuple(state, state)\n",
    "\n",
    "# with tf.variable_scope(\"lstm\", initializer=initializer) as lstm_scope:\n",
    "# if mode == \"inference\":\n",
    "#     # In inference mode, use concatenated states for convenient feeding and\n",
    "#     # fetching.\n",
    "#     tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n",
    "\n",
    "#     # Placeholder for feeding a batch of concatenated states.<\n",
    "#     state_feed = tf.placeholder(dtype=tf.float32,\n",
    "#                                 shape=[None, sum(lstm_cell.state_size)],\n",
    "#                                 name=\"state_feed\")\n",
    "#     c, h = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n",
    "#     state_tuple = LSTMStateTuple(c, h)\n",
    "\n",
    "#     # Run a single LSTM step.\n",
    "#     lstm_outputs, state_tuple = lstm_cell(inputs=tf.squeeze(seq_embeddings, axis=[1]),\n",
    "#                                           state=state_tuple)\n",
    "\n",
    "#     # Concatentate the resulting state.\n",
    "#     tf.concat(axis=1, values=state_tuple, name=\"state\")\n",
    "# else:\n",
    "# Run the batch of sequence embeddings through the LSTM.\n",
    "with tf.variable_scope(\"lstm\", initializer=initializer) as lstm_scope:\n",
    "    sequence_length = tf.reduce_sum(input_mask, 1)\n",
    "    lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                        inputs=seq_embeddings,\n",
    "                                        sequence_length=sequence_length,\n",
    "                                        initial_state=initial_state,\n",
    "                                        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Stack batches vertically.\n",
    "lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "\n",
    "with tf.variable_scope(\"logits\") as logits_scope:\n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        inputs=lstm_outputs,\n",
    "        num_outputs=len(voca), # config.vocab_size,\n",
    "        activation_fn=None,\n",
    "        weights_initializer=initializer,\n",
    "        scope=logits_scope)\n",
    "\n",
    "# if mode == \"inference\":\n",
    "#     tf.nn.softmax(logits, name=\"softmax\")\n",
    "# else:\n",
    "targets = tf.reshape(target_seqs, [-1])\n",
    "weights = tf.to_float(tf.reshape(input_mask, [-1]))\n",
    "\n",
    "# Compute losses.\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                        logits=logits)\n",
    "batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "                    tf.reduce_sum(weights),\n",
    "                    name=\"batch_loss\")\n",
    "tf.losses.add_loss(batch_loss)\n",
    "total_loss = tf.losses.get_total_loss()\n",
    "\n",
    "# Add summaries.\n",
    "tf.summary.scalar(\"losses/batch_loss\", batch_loss)\n",
    "tf.summary.scalar(\"losses/total_loss\", total_loss)\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(\"parameters/\" + var.op.name, var)\n",
    "    \n",
    "target_cross_entropy_losses = losses  # Used in evaluation.\n",
    "target_cross_entropy_loss_weights = weights  # Used in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Sets up the function to restore inception variables from checkpoint.\n",
    "# if mode != \"inference\":\n",
    "# Restore inception variables only.\n",
    "# inception_variables = tf.get_collection(\n",
    "#     tf.GraphKeys.GLOBAL_VARIABLES, scope=\"InceptionV3\")\n",
    "# saver = tf.train.Saver(inception_variables)\n",
    "\n",
    "# def restore_fn(sess):\n",
    "#     tf.logging.info(\"Restoring Inception variables from checkpoint file %s\",\n",
    "#                     inception_checkpoint_file)\n",
    "#     saver.restore(sess, inception_checkpoint_file)\n",
    "\n",
    "# init_fn = restore_fn\n",
    "# else:\n",
    "#     init_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Sets up the global step Tensor.\n",
    "global_step = tf.Variable(\n",
    " initial_value=0,\n",
    "    name=\"global_step\",\n",
    "    trainable=False,\n",
    "    collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "\n",
    "global_step = global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create training directory.\n",
    "train_dir = 'train/'\n",
    "if not tf.gfile.IsDirectory(train_dir):\n",
    "    tf.logging.info(\"Creating training directory: %s\", train_dir)\n",
    "    tf.gfile.MakeDirs(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Some important variables\n",
    "\n",
    "# Whether to train inception submodel variables.\n",
    "train_inception = False\n",
    "\n",
    "# Learning rate for the initial phase of training.\n",
    "initial_learning_rate = 2.0\n",
    "learning_rate_decay_factor = 0.5\n",
    "num_epochs_per_decay = 8.0\n",
    "\n",
    "# Number of examples per epoch of training data.\n",
    "num_examples_per_epoch = 586363\n",
    "\n",
    "# Optimizer for training the model.\n",
    "optimizer = \"SGD\"\n",
    "\n",
    "# Learning rate when fine tuning the Inception v3 parameters.\n",
    "train_inception_learning_rate = 0.0005\n",
    "\n",
    "# If not None, clip gradients to this value.\n",
    "clip_gradients = 5.0\n",
    "\n",
    "# How many model checkpoints to keep.\n",
    "max_checkpoints_to_keep = 5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the learning rate.\n",
    "learning_rate_decay_fn = None\n",
    "if train_inception:\n",
    "    learning_rate = tf.constant(train_inception_learning_rate)\n",
    "else:\n",
    "    learning_rate = tf.constant(initial_learning_rate)\n",
    "    if learning_rate_decay_factor > 0:\n",
    "        num_batches_per_epoch = (num_examples_per_epoch /\n",
    "                                 batch_size)\n",
    "        decay_steps = int(num_batches_per_epoch *\n",
    "                          num_epochs_per_decay)\n",
    "\n",
    "    def _learning_rate_decay_fn(learning_rate, global_step):\n",
    "        return tf.train.exponential_decay(\n",
    "            learning_rate,\n",
    "            global_step,\n",
    "            decay_steps=decay_steps,\n",
    "            decay_rate=learning_rate_decay_factor,\n",
    "            staircase=True)\n",
    "\n",
    "    learning_rate_decay_fn = _learning_rate_decay_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the training ops.\n",
    "train_op = tf.contrib.layers.optimize_loss(\n",
    "    loss=total_loss,\n",
    "    global_step=global_step,\n",
    "    learning_rate=learning_rate,\n",
    "    optimizer=optimizer,\n",
    "    clip_gradients=clip_gradients,\n",
    "    learning_rate_decay_fn=learning_rate_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the Saver for saving and restoring model checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "log_every_n_steps = 1\n",
    "number_of_steps = 1000000\n",
    "tf.contrib.slim.learning.train(\n",
    "    train_op,\n",
    "    train_dir,\n",
    "    log_every_n_steps=log_every_n_steps,\n",
    "    graph=sess.graph,\n",
    "    global_step=global_step,\n",
    "    number_of_steps=number_of_steps,\n",
    "    saver=saver)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# close session\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepenv)",
   "language": "python",
   "name": "deepenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "DeepLearningPipeline.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
