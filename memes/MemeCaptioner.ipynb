{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Meme Caption Generator\n",
    "<img src=\"meme_characters/futurama-fry/futurama-fry.jpg\" align=\"left\">\n",
    "<img src=\"meme_characters/philosoraptor/philosoraptor.jpg\" align=\"right\">\n",
    "<img src=\"meme_characters/y-u-no/y-u-no.jpg\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import configuration as config\n",
    "from utils.vocabulary import Vocabulary\n",
    "from utils.caption_generator import CaptionGenerator\n",
    "from utils.inception_v3 import preprocess_input\n",
    "from model import MemeModel\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from utils.inception_v3 import preprocess_input\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mod.layers.pop()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = 'model/train/'\n",
    "vocab_file = '10k/word_count.txt'\n",
    "dataset_dir = 'meme_characters/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions to generate captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(dataset_dir, image_format='jpeg'):\n",
    "    model = MemeModel('inference', vocab_file, dataset_dir=dataset_dir)\n",
    "    model.build(image_format)\n",
    "    return model\n",
    "\n",
    "def feed_image(sess, encoded_image):\n",
    "    initial_state = sess.run(fetches=\"lstm/initial_state:0\",\n",
    "                             feed_dict={\"image_feed:0\": encoded_image})\n",
    "    return initial_state\n",
    "\n",
    "def inference_step(sess, input_feed, state_feed):\n",
    "    softmax_output, state_output = sess.run(\n",
    "        fetches=[\"softmax:0\", \"lstm/state:0\"],\n",
    "        feed_dict={\n",
    "            \"input_feed:0\": input_feed,\n",
    "            \"lstm/state_feed:0\": state_feed,\n",
    "        })\n",
    "    return softmax_output, state_output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and restore model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a function that restores a model from checkpoint\n",
    "def create_restore_fn(checkpoint_path, saver):\n",
    "    if tf.gfile.IsDirectory(checkpoint_path):\n",
    "        checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n",
    "        if not checkpoint_path:\n",
    "            raise ValueError(\"No checkpoint file found in: %s\" % checkpoint_path)\n",
    "\n",
    "    def _restore_fn(sess):\n",
    "        tf.logging.info(\"Loading model from checkpoint: %s\", checkpoint_path)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        tf.logging.info(\"Successfully loaded checkpoint: %s\",\n",
    "                        os.path.basename(checkpoint_path))\n",
    "        \n",
    "    return _restore_fn\n",
    "\n",
    "# Builds the inference graph from a configuration object.\n",
    "def build_graph_from_config(data_dir, checkpoint_path, image_format='jpeg'):\n",
    "    tf.logging.info(\"Building model.\")\n",
    "    model = build_model(data_dir, image_format)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    return create_restore_fn(checkpoint_path, saver), model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and inference graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building model.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#g = tf.Graph()\n",
    "#with g.as_default():\n",
    "restore_fn, mememodel = build_graph_from_config(dataset_dir,\n",
    "                                                checkpoint_path,\n",
    "                                                image_format='jpeg')\n",
    "#g.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary.\n",
    "vocab = Vocabulary(vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run caption generation over `input_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mememodel.model.outputs = [mememodel.model.layers[-1].output]\n",
    "#mememodel.model.layers[-1].outbound_nodes = [\n",
    "mememodel.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "img = image.load_img('10k/part-0-to-1000/y-u-no/y-u-no.jpg', target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "print(x.shape)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print(x.shape)\n",
    "x = preprocess_input(x)\n",
    "print(x.shape)\n",
    "preds = mememodel.model.predict(x)\n",
    "print(preds.shape)\n",
    "#print(mememodel.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "# Load the model from checkpoint.\n",
    "restore_fn(sess)\n",
    "# end = mememodel.model.layers[-1]\n",
    "# end.output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the caption generator. Here we are implicitly using the default beam search parameters.\n",
    "See [`caption_generator.py`](utils/caption_generator.py) for a description of the\n",
    "available beam search parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = CaptionGenerator(feed_image, \n",
    "                             inference_step, \n",
    "                             vocab,\n",
    "                             max_caption_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPTION IMAGES!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Meme paths to be captioned\n",
    "input_files = ['meme_characters/american-pride-eagle/american-pride-eagle.jpg']\n",
    "\n",
    "input_files += list(map(lambda f: os.path.join('test', f), os.listdir('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in input_files:\n",
    "    if os.path.exists(filename):\n",
    "        # Display image\n",
    "        im = mpimg.imread(filename)\n",
    "        plt.imshow(im)\n",
    "        plt.figure()\n",
    "        \n",
    "        # Caption image\n",
    "        img = image.load_img(filename, target_size=(299, 299))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        #x = preprocess_input(x)\n",
    "        preds = mememodel.model.predict(x)\n",
    "        captions = generator.beam_search(sess, preds)\n",
    "        print(\"Captions for image %s:\" % os.path.basename(filename))\n",
    "        for i, caption in enumerate(captions):\n",
    "            # Ignore begin and end words.\n",
    "            print('raw sentence:', caption.sentence[1:-1])\n",
    "            sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]\n",
    "            sentence = \" \".join(sentence)\n",
    "            print(\"  %d) %s (p=%f)\" % (i, sentence, math.exp(caption.logprob)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = 'meme_characters/one-does-not-simply/one-does-not-simply.jpg'\n",
    "img = image.load_img(original_image, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "#x = preprocess_input(x)\n",
    "preds = mememodel.model.predict(x)\n",
    "#preds = np.zeros((1, 1000))\n",
    "captions = generator.beam_search(sess, preds)\n",
    "# Display image\n",
    "im = mpimg.imread(original_image)\n",
    "plt.imshow(im)\n",
    "plt.figure()\n",
    "print(\"Captions for image %s:\" % os.path.basename(original_image))\n",
    "for i, caption in enumerate(captions):\n",
    "    # Ignore begin and end words.\n",
    "    print('raw sentence:', caption.sentence[1:-1])\n",
    "    sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]\n",
    "    sentence = \" \".join(sentence)\n",
    "    print(\"  %d) %s (p=%f)\" % (i, sentence, math.exp(caption.logprob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape([x for x in preds[0] if x >= 0.15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpudeepenv",
   "language": "python",
   "name": "cpudeepenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
